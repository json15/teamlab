{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process\n",
    "- 파일을 불러오기\n",
    "- 파일을 읽어서 단어사전(corpus) 만들기\n",
    "- 단어별로 index 만들기\n",
    "- 만들어진 인덱스로 문서별로 bag of words vector 생성\n",
    "- 비교하고자 하는 문서 비교하기\n",
    "- 얼마나 맞는지 측정하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파일 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is listdir?\n",
    "# it returns the files and directories inside the dir, as type list\n",
    "def get_file_list(dir_name):\n",
    "    return os.listdir(dir_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_list에 리스트로 반환된 news_data dir 하위의 파일을 list type으로 return\n",
    "# Why use os.path.join?\n",
    "# dir이름과 하위 파일을 합치고 싶을때, os 별로 나누는 방법이 다르기 때문에 현재 사용하고\n",
    "#있는 os에 맞춰서 연결시켜준 이름을 뽑기위해서 쓴다\n",
    "target_dir = 'news_data'\n",
    "file_list = get_file_list(target_dir)\n",
    "file_list = [os.path.join(target_dir, file_name) for file_name in file_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파일별로 내용 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_class에는 해당 index의 신문이 1~8중 어느것이고, 축구인지, 야구인지 구별하는 1,0이 dictionary\n",
    "# type 으로 들이간다\n",
    "# X_texts는 해당 인덱스의 문서의 모든 내용이 들어간다\n",
    "\n",
    "# class_dict로 1~8까지의 카테고리중 어느것이 축구, 야구인지 구별해준다\n",
    "\n",
    "# f로 파일을 열어주고, encoding 은 cp949로 해준다, windows로 작성한 파일이기 때문이다\n",
    "\n",
    "# category 우선 file_name (news_data/1_news about bla blabla)을 가져온후 split 해준다\n",
    "# os.sep을 쓰는 이유는 dir_name: file_name으로 split 해주어야하는데 os 별로 seporation\n",
    "# 방법이 다르기 때문이다. 이중 2번째  file_name만 가져온후 _ 로 다시 seporate 해주어서\n",
    "# 파일명 재일 앞의 번호만 가져온다, 그것을 int type 으로 return!\n",
    "\n",
    "# y_class category에서 뽑아온 int를 key값으로 사용해서 해당 value를 뽑아서 넣어준다\n",
    "\n",
    "\n",
    "\n",
    "def get_conetents(file_list):\n",
    "    y_class = []\n",
    "    X_class = []\n",
    "    class_dict = {\n",
    "        1: '0', 2: '0', 3: '0', 4: '0', 5: '1', 6: '1', 7: '1', 8: '1'\n",
    "    }\n",
    "    for file_name in file_list:\n",
    "        try:\n",
    "            f = open(file_name, 'r', encoding= 'cp949')\n",
    "            category = int(file_name.split(os.sep)[1].split('_')[0])\n",
    "            y_class.append(class_dict[category])\n",
    "            X_text.append(f.read())\n",
    "        except UnicodeDecodeError as e:\n",
    "            print(e)\n",
    "            print(file_name)\n",
    "    return X_text, y_class\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# corpus 만들기 + 단어별 index 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_cleaned_text -> 문장속에 있는 각종 특수문자들을 모두 제거 해준 뒤에, \n",
    "# 소문자로 변환 \n",
    "\n",
    "# get_corpus_dict -> x_text에 들어있는 모든 단어들에 관한 corpus dict를 \n",
    "# 만든다\n",
    "\n",
    "# text는 모든 기사들 중 하나의 기사씩 뽑은 후에 그기사를 단어별로 split 해주어\n",
    "# 2 dementional array 를 생성한다, [[단어1], [단어2]~~]\n",
    "\n",
    "# cleaned_words는 위의 2 dimentional array를 쭉 펴주어서 1 dimentional\n",
    "# array 로 만들어준다, get_cleaned_text또한 통과하기때문에, 특수문자가 없는\n",
    "# 소문자 단어리스트를 생성해 준다\n",
    "\n",
    "# corpus_dict 순서가 있는 dictionary로 만들어준후, set()를 통과해 중복\n",
    "# 값이 없어진 cleaned_words를 key 값으로, value로 0부터 숫자를 먹여 준다\n",
    "\n",
    "\n",
    "def get_cleaned_text(text):\n",
    "    import re\n",
    "    text = re.sub('\\W+', '', text.lower())\n",
    "    return text\n",
    "\n",
    "def get_corpus_dict(text):\n",
    "    text = [sentence.split() for sentence in text]\n",
    "    cleaned_words = [get_cleaned_text(word) for words in text\n",
    "                    for word in words]\n",
    "    \n",
    "    from collections import OrderedDict\n",
    "    corpus_dict = OrderedDict()\n",
    "    for i, v in enumerate(set(cleaned_words)):\n",
    "        corpus_divt[v] = i\n",
    "    return corpus_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 문서별로 Bag of words 생성\n",
    "각 뉴스 별로 존제 하는 단어들의 index를 저장 해놓은 matrix를 생성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text는 모든 뉴스가 담겨있는 matrix, corpus는 앞에서 만든 모든 뉴스의 모든 단어에 대한\n",
    "# list\n",
    "\n",
    "# text = 모든 기사들 중 하나의 기사씩 뽑은 후에 그기사를 단어별로 split 해주어\n",
    "# 2 dementional array 를 생성한다, [[단어1], [단어2]~~]\n",
    "\n",
    "# word_number_list = words -> 뉴스 하나의 단어 리스트, word -> 그리스트중 한게의 단어 '\n",
    "# 그단어를 깨끗하게 정리한후, corpus에서 해당 단어의 index를 찾아 넣어준다\n",
    "\n",
    "# X_vector = corpus 의 길이 만큼의 vector를 text의 갯수만큼 생성한 matrix를 생성\n",
    "# 해준다\n",
    "\n",
    "# for loop\n",
    "# word_number_list에서 각 뉴스별로 가지고 있는 index list 를 뽑아준다\n",
    "# word_number는 그중 하나의 인덱스\n",
    "# X_vector의 첫번째 list에 해당 인덱스가 나올때 마다 1씩 추가해준다\n",
    "# 그 X_vector를 return\n",
    "def get_count_vector(text, corpus):\n",
    "    text = [sentence.split() for sentence in text]\n",
    "    word_number_list = [[corpus[get_cleaned_text(word)] for word in words]\n",
    "                        for words in text]\n",
    "    X_vector = [[0 for _ in range(len(corpus))] for x in range(len(text))]\n",
    "    \n",
    "    for i, text in enumerate(word_number_list):\n",
    "        for word_number in text:\n",
    "            X_vector[i][word_number] += 1\n",
    "    \n",
    "    return X_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비교하기 cosine Similarity 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_cosine_similarity(v1, v2):\n",
    "    '''comput cosine similarity of v1 to v2: (v1 dot v2) / (||v1|| * \n",
    "    ||v2||)'''\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0,\n",
    "    for i in range(len(v1)):\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x * x\n",
    "        sumxy += x * y\n",
    "        sumyy += y * y\n",
    "    return sumxy / math.sqrt(sumxx * sumyy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 비교결과 정리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "get_similarity_score\n",
    "    X_vector에 모든 문서에 대한 bag_of_words를 가져오고, source에 비교하고싶은 문서의\n",
    "    인덱스를 입력해준다\n",
    "    source_vector = 가져와서 비교해보고싶은 기본 문서\n",
    "    similarity_list에 모든 문서에 대한 비교 결과를 넣는다\n",
    "    for loop에서 모든 문서에대한 cosince similarity를 얻고, 그값을 similarity list\n",
    "    에 append싴켜준다\n",
    "    \n",
    "get_top_n_similarity_news\n",
    "    enumerate 로 위에서 계산한 모든 similarity score과 index가 들어있는 dict를 \n",
    "    만들어주고, index가 key 값으로 들어간다\n",
    "    \n",
    "    sorted_x에서 정렬을 해주고, 큰수부터 필요하기 때문에 reversed해주어야 한다\n",
    "'''\n",
    "\n",
    "def get_similarity_score(X_vector, source):\n",
    "    source_vector = X_vector[source]\n",
    "    similarity_list = []\n",
    "    for target_vector in X_vector:\n",
    "        similarity_list.append(\n",
    "        get_cosince_similarity(source_vector, target_vector))\n",
    "    return similarity_list\n",
    "\n",
    "\n",
    "def get_top_n_similarity_news(similarity_score, n):\n",
    "    import operator\n",
    "    x = {i:v for i,v in enumerate(similarity_score)}\n",
    "    sorted_x = sorted(x.items(), key=operator.itemgetter(1))\n",
    "    \n",
    "    return list(reversed(sorted_x))[1:n+1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
